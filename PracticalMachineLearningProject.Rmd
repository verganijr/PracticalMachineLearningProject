---
title: "Practical Machine Learning Project"
output:
  html_document: default
  pdf_document: default
---
###Executive Summary
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks

The goal of this project is to predict the manner in which they did the exercise. This is the "class" variable in the training set. It is possible to use any of the other variables to predict with. This report describes how to build the model, how to use cross validation, what the expected out of sample error is, and the choices made based on the models evaluated.

Note: The data for this project come from this source:

training data: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

test data: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

#####Libraries used in this project/report
```{r warning=FALSE}
if (!require('caret'))
  install.packages("caret")
if (!require('ggplot2'))
  install.packages("ggplot2")
if (!require('rpart'))
  install.packages("rpart")
if (!require('rpart.plot'))
  install.packages("rpart.plot")
if (!require('rattle'))
  install.packages("rattle")
if (!require('randomForest'))
  install.packages("randomForest")
if (!require('e1071'))
  install.packages("e1071")
library(ggplot2)
library(caret)
library(lattice)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
library(randomForest)
library(e1071)

```

###Loading data sets

```{r}
setwd("C:/Data Science/Practical-Machine-Learning-master/Practical-Machine-Learning")

training <- read.csv("pml-training.csv",na.strings=c("NA","#DIV/0!", ""))
```
{r, echo=FALSE}
dim ( training  )
```
```{r}
testing <- read.csv("pml-testing.csv",na.strings=c("NA","#DIV/0!", ""))
```
{r, echo=FALSE}
dim ( testing  )
```
#The training set consists of 19622 observations of 160 variables
#The testing set consists of 20 observations of 160 variables

```
-The training set consists of 19622 observations of 160 variables

-The testing set consists of 20 observations of 160 variables

### Cleaning data
Columns in the original training and testing datasets that are mostly filled with missing values are then removed count the number of missing values in each column of the full training dataset
```{r}
training <-training[,colSums(is.na(training)) == 0]
testing <-testing[,colSums(is.na(testing)) == 0]
```

```{r}
training   <-training[,-c(1:7)]
testing <-testing[,-c(1:7)]

dim ( training ) 

dim ( testing  )
```


###Zero variance predictors
Diagnoses predictors that have one unique value (i.e. are zero variance predictors) or predictors that are have both of the following characteristics
```{r}

ColumnsZVar <- nearZeroVar(training, saveMetrics = TRUE)
training <- training[, ColumnsZVar$nzv==FALSE]
training$classe = factor(training$classe)
```


Partitioning the training data This validation dataset will allow us to perform cross validation when developing our model.


### Partitioning the training data set to allow cross-validation
```{r}
set.seed(1234)
subTrain <- createDataPartition(y=training$classe, p=.75, list=FALSE)

TheTraining <- training[subTrain, ]
TheTesting <- training[-subTrain, ]
```


Dataset contains 59 variables, with the last column containing the 'class' variable we are trying to predict.

### Modelprediction 1 : Using Decision Tree
```{r}
modelDT <- rpart(classe ~ ., data=TheTraining, method="class")
```

```{r}
predictionDT <- predict(modelDT, TheTesting, type ="class")
```

### Plot of the Decision Tree
```{r}
rpart.plot(modelDT, main="Decision Tree ", extra=102, under=TRUE, faclen=0)
fancyRpartPlot (modelDT, main="Decision Tree")
```

###Test results on our subTesting data set:
```{r}
confusionMatrix(predictionDT,TheTesting$classe)
```
The Confusion Matrix achieved  0.7394 % accuracy. Here, the 95% CI : (0.7269, 0.7516). The Kappa statistic of  0.6697  reflects the out-of-sample error. 
For the above values is necessary to use the method Random Forest Model determineis much better estimator and predictor.



Applied the Random Forest Model and it has shown significant amount of accuracy in prediction.
###Modelprediction 2 : Using Random Forest

```{r}
modelRF <- randomForest(classe ~. , data=TheTraining, method="class")
print (modelRF)
```
### Predicting:

```{r}
predictionRF <- predict(modelRF, TheTesting, type = "class")
```
### Test results on subTesting data set:
```{r, echo=TRUE}
confusionMatrix(predictionRF, TheTesting$classe)
```
The Confusion Matrix achieved 99.51% accuracy.in the 95% CI : (0.9927, 0.9969)  and the OOB (Out-Of-Bag) Error Rate is 0.43%.The Kappa statistic of  0.9938  reflects the out-of-sample error


###Decision

As expected, Random Forest algorithm performed better than Decision Trees.
Accuracy for Random Forest model was Accuracy : 0.9951  and (95% CI: ((0.9927, 0.9969))) compared to 95% CI : (0.7269, 0.7516) for Decision Tree model. 
The random Forest model is choosen. The accuracy of the model is 0.995. The expected out-of-sample error is estimated at 0.005, or 0.5%. The expected out-of-sample error is calculated as 1 - accuracy for predictions made against the cross-validation set. Our Test data set comprises 20 cases. With an accuracy above 99% on our cross-validation data, we can expect that very few, or none, of the test samples will be missclassified.

###Conclusion

Of the two Prediction Methods used in the study, accuracy was better method The Random Forest method since the Confusion Matrix achieves approximately 

This model will be used for the final calculations is the project.


###Submission


```{r}
  pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i], file = filename, quote = FALSE, row.names = FALSE, col.names = FALSE)
  }
}
pml_write_files(predict(modelRF, testing[, -length(names(testing))]))
rm(modelRF)
rm(training)
rm(testing)
rm(validation)
rm(pml_write_files)
```


